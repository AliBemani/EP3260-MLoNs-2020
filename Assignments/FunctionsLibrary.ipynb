{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __main__ import *\n",
    " \n",
    "def sigmoid(x, derivative=False):\n",
    "    sigm = 1. / (1. + np.exp(-x))\n",
    "    if derivative:\n",
    "        return sigm * (1. - sigm)\n",
    "    return sigm\n",
    " \n",
    " \n",
    "def gradient_single(X, y, w, lmbda):\n",
    "    # compute gradient with respect to a single sample and assign to output\n",
    " \n",
    "    return output\n",
    " \n",
    " \n",
    "def f_gradient(X, y, w):\n",
    "    yX = y[:, np.newaxis] * X\n",
    "    l1 = np.matmul(yX, w)\n",
    "    return -sigmoid(-l1)[:, np.newaxis] * yX\n",
    " \n",
    " \n",
    "def reg_gradient(w, lmbda):\n",
    "    return 2 * lmbda * w\n",
    " \n",
    " \n",
    "def gradient(X, y, w, lmbda):\n",
    "    return reg_gradient(w, lmbda) + np.mean(f_gradient(X, y, w), axis=0)\n",
    " \n",
    " \n",
    "def gradient_full(X, y, w, lmbda):\n",
    "    # Compute the full gradient of the given function \n",
    "    \n",
    "    return output\n",
    " \n",
    " \n",
    "def cost(X, y, w, lmbda):\n",
    "     # Define the cost function\n",
    " \n",
    "    return output\n",
    " \n",
    "def binary_classification_cost(X, y, w):\n",
    "    return f1_score(y_true = y, y_pred = np.sign(np.matmul(X, w)), average='micro')\n",
    " \n",
    "def ax_modifier(ax, legend_loc, ncol, xlabel, ylabel, title=None):\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    xmin, xmax = ax.get_xlim()\n",
    "    no_curves = len(ax.lines)\n",
    "    ymin = min(ymin, min([min(ax.lines[i].get_ydata()) for i in range(no_curves)]))\n",
    "    ymax = max(ymax, max([max(ax.lines[i].get_ydata()) for i in range(no_curves)]))\n",
    "    xmax = max(xmax, max([max(ax.lines[i].get_xdata()) for i in range(no_curves)]))\n",
    "    ax.legend(loc=legend_loc)\n",
    "    ax.set_xlabel(xlabel, fontsize=16)\n",
    "    ax.set_ylabel(ylabel, fontsize=16)\n",
    "    ax.set_xticks(np.arange(0, xmax+1, step=round(xmax/15)))\n",
    "    ax.legend(ncol=ncol)\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    if not(np.isinf([ymin, ymax]).all()):  #if boundaries are defined\n",
    "        ax.set_ylim((0.98*ymin, 1.02*ymax))\n",
    " \n",
    " \n",
    "def random_sampler(N, batch=1, buffersize=10000):\n",
    "    \"\"\"\n",
    "    A generator of random indices from 0 to N.\n",
    "    params:\n",
    "    N: upper bound of the indices\n",
    "    batch: Number of indices to return per iteration\n",
    "    buffersize: Number of numbers to generate per batch\n",
    "                (this is only a computational nicety)\n",
    "    \"\"\"\n",
    " \n",
    "    S = int(np.ceil(buffersize / batch))\n",
    " \n",
    "    while True:\n",
    "        buffer = np.random.randint(N, size=(S, batch))\n",
    "        for i in range(S):\n",
    "            yield buffer[i]\n",
    " \n",
    " \n",
    "def GD(X, y, w, learning_rate=0.1, lmbda=0.01, iterations=1000):\n",
    "    for iteration in range(iterations):\n",
    "        # update the parameter w\n",
    " \n",
    "        yield w\n",
    " \n",
    " \n",
    "def SGD(X, y, w, lmbda, learning_rate, batch_size=1):\n",
    "    N, D = X.shape\n",
    "    sampler = random_sampler(N, batch_size)\n",
    " \n",
    "    for ix in sampler:\n",
    "       # update the parameter w \n",
    "    \n",
    "        yield w\n",
    " \n",
    " \n",
    "def SVRG(X, y, w, lmbda, learning_rate, epoch_size):\n",
    "    \"\"\"\n",
    "    Stochastic variance reduced gradient\n",
    "    \"\"\"\n",
    " \n",
    "    sampler = random_sampler(X.shape[0], epoch_size)\n",
    " \n",
    "    for epoch in sampler:\n",
    "        full_grad = gradient_full(X, y, w, lmbda)\n",
    "        mean_grad = np.mean(full_grad, axis=0)\n",
    " \n",
    "        # write inner loop of SVRG using full gradient vectors computed in full_grad and its mean value mean_grad\n",
    "    \n",
    " \n",
    "        yield w\n",
    " \n",
    " \n",
    "def SAG(X, y, w, lmbda, learning_rate, batch_size=1):\n",
    "    N, D = X.shape\n",
    "    P, = w.shape\n",
    "    sampler = random_sampler(N, batch_size)\n",
    " \n",
    "    grad = np.zeros((N, P))\n",
    "    delta = np.zeros(P)\n",
    "    non_zero_v = np.zeros(N)\n",
    "    m = 0\n",
    "    for ix in sampler:\n",
    "        # update the number of seen examples m\n",
    "        m -= np.sum(non_zero_v[ix], axis=0)\n",
    "        non_zero_v[ix] = 1\n",
    "        m += np.sum(non_zero_v[ix], axis=0)\n",
    " \n",
    "        # complete the update the sum of the gradient\n",
    "      \n",
    "         # update the parameter w \n",
    "        yield w\n",
    " \n",
    " \n",
    "def initialize_w(N):\n",
    "    return np.random.randn(N)\n",
    " \n",
    " \n",
    "def loss(X, y, w, lmbda):\n",
    "    objective_loss = cost(X, y, w, lmbda)\n",
    "    # compute f1_score\n",
    "    \n",
    "    return objective_loss, f1_score\n",
    " \n",
    " \n",
    "def iterate(opt, X_train, y_train, X_test, y_test, w_0,\n",
    "            lmbda, iterations=100, inner=1, name=\"NoName\", printout=True):\n",
    "    \"\"\"\n",
    "    This function takes an optimizer and returns a loss history for the\n",
    "    training and test sets.\n",
    "    \"\"\"\n",
    " \n",
    "    loss_hist_train, train_f1_score = loss(X_train, y_train, w_0, lmbda)\n",
    "    loss_hist_test, test_f1_score = loss(X_test, y_test, w_0, lmbda)\n",
    "    mean_grad_norm = np.linalg.norm(np.mean(gradient_full(X_train, y_train, w_0, lmbda), axis=0), 2)\n",
    " \n",
    "    ws = [w_0]\n",
    "    clock = [0]\n",
    " \n",
    "    start = time.time()\n",
    "    for iteration in range(iterations):\n",
    "        #print(\"\\n ------Iteration {} is started-----\".format(iteration))\n",
    "        for _ in range(inner):\n",
    "            w = next(opt)\n",
    "        clock.append(time.time() - start)\n",
    "        ws.append(w)\n",
    " \n",
    "    #for iteration, w in enumerate(ws):\n",
    "        train_loss, train_f1_score_new = loss(X_train, y_train, w, lmbda)\n",
    "        loss_hist_train = np.append(loss_hist_train, train_loss)\n",
    "        train_f1_score = np.append(train_f1_score, train_f1_score_new)\n",
    " \n",
    "        test_loss, test_f1_score_new = loss(X_test, y_test, w, lmbda)\n",
    "        loss_hist_test = np.append(loss_hist_test, test_loss)\n",
    "        test_f1_score = np.append(test_f1_score, test_f1_score_new)\n",
    "        grad_norm_new = np.linalg.norm(np.mean(gradient_full(X_train, y_train, w, lmbda), axis=0), 2)\n",
    "        mean_grad_norm = np.append(mean_grad_norm, grad_norm_new)\n",
    " \n",
    "        if printout:\n",
    "            print('{}; Iter = {:02}; Objective(train) = {:05.3f}; Objective(test) = {:05.3f}; F1score(train) = {:05.3f}; F1score(test) = {:05.3f}'.format(name, iteration, train_loss, test_loss, train_f1_score_new, test_f1_score_new))\n",
    "        #print('Current solution = {}'.format(w.round(decimals=3)))\n",
    "        sys.stdout.flush()\n",
    " \n",
    "    return ws[-1], loss_hist_train, loss_hist_test, train_f1_score, test_f1_score, clock, mean_grad_norm\n",
    " \n",
    " \n",
    "def optimizer_binary_classification(target_dataset, X_train, y_train,\n",
    "                                                    X_test, y_test, w_0,\n",
    "                                                    hyper_parameters, multi_class=False, printout=True):\n",
    "    optimizers = [\n",
    "         {\n",
    "                    \"opt\": GD(X=X_train, y=y_train, w=w_0, lmbda=hyper_parameters['lambda'],\n",
    "                              learning_rate=hyper_parameters['learning_rate'],\n",
    "                              iterations = hyper_parameters['iterations']),\n",
    "                    \"name\": \"GD\",\n",
    "                    \"inner\": 1\n",
    "            },\n",
    "            {\n",
    "                # complete SGD\n",
    "                   # \"opt\": SGD() \n",
    "            },\n",
    "            {\n",
    "                # complete SAG\n",
    "                  #  \"opt\": SAG()\n",
    "            },\n",
    "            {\n",
    "                # complete SVRG\n",
    "                  #  \"opt\": SVRG()\n",
    "            },\n",
    "           \n",
    "    ]\n",
    " \n",
    "    outputs = {optimizers[i]['name']: {'optimal_parameter': {}, 'training_loss': {}, 'training_f1_score': {}, 'training_mean_grad_norm': {}}\n",
    "               for i in np.arange(len(optimizers))}\n",
    " \n",
    "    fig, ax = plt.subplots(3, 1, figsize=(13, 12))\n",
    " \n",
    "    for opt in optimizers:\n",
    " \n",
    "        w, loss_hist_train, loss_hist_test, train_f1_score, test_f1_score, clock, mean_grad_norm = iterate(\n",
    "                opt['opt'],\n",
    "                X_train, y_train, X_test, y_test, w_0,\n",
    "                lmbda=hyper_parameters['lambda'],\n",
    "                iterations=hyper_parameters['iterations'], inner=opt['inner'],\n",
    "                name=opt['name'], printout=printout)\n",
    "        outputs[opt['name']]['optimal_parameter'] = w\n",
    "        outputs[opt['name']]['training_loss'] = loss_hist_train\n",
    "        outputs[opt['name']]['training_f1_score'] = train_f1_score\n",
    "        outputs[opt['name']]['training_mean_grad_norm'] = mean_grad_norm\n",
    " \n",
    "        color = next(ax[0]._get_lines.prop_cycler)['color']\n",
    "        iterations_axis = range(0, hyper_parameters['iterations'] + 1)\n",
    "        ax[0].plot(iterations_axis, loss_hist_train,\n",
    "               label=\"Train loss ({})\".format(opt['name']), linestyle=\"-\", color=color)\n",
    " \n",
    "        ax[0].plot(iterations_axis, loss_hist_test,\n",
    "               label=\"Test loss ({})\".format(opt['name']), linestyle=\"--\", color=color)\n",
    " \n",
    "        ax[1].plot(iterations_axis, mean_grad_norm,\n",
    "               label=\"Training ({})\".format(opt['name']), linestyle=\"-\", color=color)\n",
    " \n",
    "        #ax[2].plot(iterations_axis, train_f1_score,\n",
    "        #       label=\"Train F1 score ({})\".format(opt['name']), linestyle=\"-\", color=color)\n",
    " \n",
    "        ax[2].plot(iterations_axis, test_f1_score,\n",
    "               label=\"{}\".format(opt['name']), linestyle=\"-\", color=color)\n",
    " \n",
    "        #ax[3].plot(clock, loss_hist_train,\n",
    "        #           label=\"Train loss ({})\".format(opt['name']), linestyle=\"-\", color=color)\n",
    "        #ax[3].plot(clock, loss_hist_test,\n",
    "        #           label=\"Test loss ({})\".format(opt['name']), linestyle=\"--\", color=color)\n",
    " \n",
    "    ax_modifier(ax=ax[0], legend_loc=\"upper right\", ncol=2, xlabel=\"Iteration\", ylabel=\"Loss\",\n",
    "                title=\"Performance Comparison of various algorithms\")\n",
    "    ax_modifier(ax=ax[1], legend_loc=\"upper right\", ncol=2, xlabel=\"Iteration\", ylabel=\"Gradient norm (training)\")\n",
    "    ax_modifier(ax=ax[2], legend_loc=\"lower right\", ncol=2, xlabel=\"Iteration\", ylabel=\"F1 score (test)\")\n",
    "    #ax_modifier(ax=ax[3], legend_loc=\"upper right\", xlabel=\"Time in seconds\", ylabel=\"Loss\")\n",
    " \n",
    "    if multi_class:\n",
    "        fig_name = './TestResults/noQuant_allAlg_'+target_dataset+'_Class'+str(y_train[0])\n",
    "    else:\n",
    "        fig_name = './TestResults/noQuant_allAlg_'+target_dataset+'_BinaryClassification'\n",
    " \n",
    "    plt.savefig(fig_name+'.png')\n",
    "    plt.savefig(fig_name+'.pdf')\n",
    "    matplotlib2tikz.save(fig_name+'.tex')\n",
    " \n",
    "    return outputs"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
